{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1iKDNgCwVRW"
      },
      "source": [
        "# ML4DD Summer School Hackathon\n",
        "\n",
        "The final days of the Machine Learning For Drug Discovery summer school ends with a hackathon. We will use Polaris as a tool to get the associated benchmarks and datasets. First things first, we will install Polaris from PyPi."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_IWEa_YxPrp"
      },
      "source": [
        "We next need to authenticate ourselves to Polaris. If you haven't done so yet, you can create an account at https://polarishub.io. Afterwards, you can simply run the command below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iujt4e23r1M",
        "outputId": "40cae3eb-aee6-4bb3-a2f4-66536de12421"
      },
      "outputs": [],
      "source": [
        "# Use the organization owner settings\n",
        "owner = \"team13\"\n",
        "\n",
        "print(f'You have set \"{owner}\" as the owner')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3MLaBrY5Wn0"
      },
      "outputs": [],
      "source": [
        "import polaris as po\n",
        "import datamol as dm\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TO5OAEVFyuqG"
      },
      "source": [
        "# Kinase Selectivity\n",
        "\n",
        "The second benchmark we will use is `polaris/pkis1-kit-wt-mut-c-1`. Using this benchmark is very similar to before, except for one difference: This is a multi-task benchmark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46Y3c1JMIPLh",
        "outputId": "f487c4cd-9eac-48cd-96b4-0e284b03d652"
      },
      "outputs": [],
      "source": [
        "benchmark = po.load_benchmark(\"polaris/pkis1-kit-wt-mut-c-1\")\n",
        "train, test = benchmark.get_train_test_split()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pca2oqUVzJ6f"
      },
      "source": [
        "As we can see, the targets are now returned to us as a dictionary. Let's train a multi-task model on this data! We first preprocess the data to be in a format we can use with scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMVcMtmyzh6g",
        "outputId": "9aa916ab-66e5-4903-81fb-d95c24f057ea"
      },
      "outputs": [],
      "source": [
        "ys = train.y\n",
        "ys = np.stack([ys[target] for target in benchmark.target_cols], axis=1)\n",
        "ys.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hCe_IEr0f1q"
      },
      "source": [
        "Now that we're working with a multi-task dataset, it's also possible for these arrays to be sparse. Let's filter out any data points that doesn't have readouts for _all_ targets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sc6-h6e-0fS0",
        "outputId": "1e8ede36-c85c-48a5-b882-9e4a04c2dc30"
      },
      "outputs": [],
      "source": [
        "mask = ~np.any(np.isnan(ys), axis=1)\n",
        "mask.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train = pd.DataFrame(train.X[mask])\n",
        "df_train.columns = [\"smiles\"]\n",
        "df_train[benchmark.target_cols] = ys[mask]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Add phyisical features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "features = [\n",
        "    \"MolecularWeight\",\n",
        "    \"LogP\",\n",
        "    \"MaxAbsPartialCharge\",\n",
        "    \"MinAbsPartialCharge\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.utils import featurize_smiles\n",
        "\n",
        "df_train[features] = df_train[\"smiles\"].apply(lambda x: pd.Series(featurize_smiles(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = df_train[features].values\n",
        "y = df_train[benchmark.target_cols].values\n",
        "\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Do the same featurization on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_test = pd.DataFrame(test.X)\n",
        "df_test.columns = [\"smiles\"]\n",
        "df_test[features] = df_test[\"smiles\"].apply(lambda x: pd.Series(featurize_smiles(x)))\n",
        "\n",
        "X_test = df_test[features].values\n",
        "print(\"X_test shape:\", X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Baseline with Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Construct a random forest regressor for each target\n",
        "models = {\n",
        "    target: RandomForestClassifier(max_depth=5) for target in benchmark.target_cols\n",
        "}\n",
        "\n",
        "# Train the models\n",
        "for target in benchmark.target_cols:\n",
        "    models[target].fit(X, y[:, benchmark.target_cols.index(target)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predict the test set\n",
        "y_prob_rf = {\n",
        "    target: model.predict_proba(X_test)[:, 1] for target, model in models.items()\n",
        "}\n",
        "y_pred_rf = {target: model.predict(X_test) for target, model in models.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = benchmark.evaluate(y_pred=y_pred_rf, y_prob=y_prob_rf)\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multioutput"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.multioutput import MultiOutputClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_model = RandomForestClassifier()\n",
        "model = MultiOutputClassifier(base_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predict the test set\n",
        "y_pred_mrf = model.predict(X_test)\n",
        "y_prob_mrf = model.predict_proba(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred_mrf = {\n",
        "    target: y_pred_mrf[:, i] for i, target in enumerate(benchmark.target_cols)\n",
        "}\n",
        "y_prob_mrf = {\n",
        "    target: y_prob_mrf[i][:, 1] for i, target in enumerate(benchmark.target_cols)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(benchmark.target_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predict for class 1\n",
        "print(\"Predicted as binders for target 1:\", y_pred_mrf[benchmark.target_cols[0]].sum())\n",
        "print(\"Predicted as binders for target 2:\", y_pred_mrf[benchmark.target_cols[1]].sum())\n",
        "print(\"Predicted as binders for target 3:\", y_pred_mrf[benchmark.target_cols[2]].sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = benchmark.evaluate(y_pred=y_pred_mrf, y_prob=y_prob_mrf)\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Resampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "smote = SMOTE()\n",
        "\n",
        "resampled_datasets = {}\n",
        "\n",
        "for target in benchmark.target_cols:\n",
        "    X_resampled, y_resampled = smote.fit_resample(\n",
        "        X, y[:, benchmark.target_cols.index(target)]\n",
        "    )\n",
        "    resampled_datasets[target] = (X_resampled, y_resampled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "models = {target: RandomForestClassifier() for target in benchmark.target_cols}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for target in benchmark.target_cols:\n",
        "    models[target].fit(*resampled_datasets[target])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred_rf_resampled = {\n",
        "    target: model.predict(X_test) for target, model in models.items()\n",
        "}\n",
        "y_prob_rf_resampled = {\n",
        "    target: model.predict_proba(X_test)[:, 1] for target, model in models.items()\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predict for class 1\n",
        "print(\n",
        "    \"Predicted as binders for target 1:\",\n",
        "    y_pred_rf_resampled[benchmark.target_cols[0]].sum(),\n",
        ")\n",
        "print(\n",
        "    \"Predicted as binders for target 2:\",\n",
        "    y_pred_rf_resampled[benchmark.target_cols[1]].sum(),\n",
        ")\n",
        "print(\n",
        "    \"Predicted as binders for target 3:\",\n",
        "    y_pred_rf_resampled[benchmark.target_cols[2]].sum(),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = benchmark.evaluate(y_pred=y_pred_rf_resampled, y_prob=y_prob_rf_resampled)\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define an MLP model, then train it with a loss which is proportional to the class imbalance\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_layers, output_dim):\n",
        "        super(MLP, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_layers = hidden_layers\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.layers.append(nn.Linear(input_dim, hidden_layers[0]))\n",
        "        for i in range(1, len(hidden_layers)):\n",
        "            self.layers.append(nn.Linear(hidden_layers[i - 1], hidden_layers[i]))\n",
        "        self.layers.append(nn.Linear(hidden_layers[-1], output_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers[:-1]:\n",
        "            x = torch.relu(layer(x))\n",
        "        x = torch.sigmoid(self.layers[-1](x))\n",
        "        return x\n",
        "\n",
        "\n",
        "# Define the loss function\n",
        "class WeightedBCELoss(nn.Module):\n",
        "    def __init__(self, pos_weight):\n",
        "        super(WeightedBCELoss, self).__init__()\n",
        "        self.pos_weight = pos_weight\n",
        "        self.bce = nn.BCELoss(reduction=\"mean\")\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        if y_pred > 0.5:\n",
        "            return self.bce(y_pred, y_true) * self.pos_weight\n",
        "        else:\n",
        "            return self.bce(y_pred, y_true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "weights = {}\n",
        "\n",
        "for target in benchmark.target_cols:\n",
        "    num_positive = y[:, benchmark.target_cols.index(target)].sum()\n",
        "    num_negative = y.shape[0] - num_positive\n",
        "    pos_weight = num_negative / num_positive\n",
        "    weights[target] = pos_weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_epochs = 100\n",
        "hidden_layers = [64]\n",
        "\n",
        "# Define the model\n",
        "models = {\n",
        "    target: MLP(input_dim=X.shape[1], hidden_layers=hidden_layers, output_dim=1)\n",
        "    for target in benchmark.target_cols\n",
        "}\n",
        "\n",
        "for target in benchmark.target_cols:\n",
        "    model = models[target]\n",
        "    model.train()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = WeightedBCELoss(pos_weight=weights[target])\n",
        "\n",
        "    X_t = torch.tensor(X, dtype=torch.float32)\n",
        "    y_t = torch.tensor(y[:, benchmark.target_cols.index(target)], dtype=torch.float32)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(X_t).squeeze()\n",
        "        loss = criterion(y_pred, y_t)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predict the test set\n",
        "y_prob_mlp = {\n",
        "    target: model(torch.tensor(X_test, dtype=torch.float32)).detach().numpy().squeeze()\n",
        "    for target, model in models.items()\n",
        "}\n",
        "\n",
        "y_pred_mlp = {\n",
        "    target: (y_prob > 0.5).astype(int) for target, y_prob in y_prob_mlp.items()\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Predicted as binders for target 1:\", y_pred_mlp[benchmark.target_cols[0]].sum())\n",
        "print(\"Predicted as binders for target 2:\", y_pred_mlp[benchmark.target_cols[1]].sum())\n",
        "print(\"Predicted as binders for target 3:\", y_pred_mlp[benchmark.target_cols[2]].sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = benchmark.evaluate(y_pred=y_pred_mlp, y_prob=y_prob_mlp)\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Upload results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhSM7C4bONxc"
      },
      "outputs": [],
      "source": [
        "results.name = \"my-second-result\"\n",
        "results.description = \"ECFP fingerprints with a Random Forest\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_WeO8Ay3eYu",
        "outputId": "cdf09690-4f47-4ab4-fc67-882934d037a4"
      },
      "outputs": [],
      "source": [
        "# results.upload_to_hub(owner=owner)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ML2Say8Q5aWn"
      },
      "source": [
        "The End."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
